{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADA-SITE-JML/azsl_dataloader/blob/main/AzSL_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaMOmJAf2Xn4"
      },
      "source": [
        "In this notebook, I took out the feature extractor (a pre-trained model) from the Data Loader and added it into the Encoder. The name of the notebook also holds this: *_TRansferLEarningInENCoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StQdMEpvC6Vr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sklearn.utils\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgiZx1tgjOSw"
      },
      "source": [
        "In \"Local\"  option, the notebook shall connect to the local machine (where the  dataset is supposed to be). To connect to the CeDAR's environment run the following to start Jupyter with access:\n",
        "\n",
        "```\n",
        "jupyter notebook \\\n",
        ">   --NotebookApp.allow_origin='https://colab.research.google.com' \\\n",
        ">   --port=8888 \\\n",
        ">   --NotebookApp.port_retries=0\n",
        "```\n",
        "Then select \"Connect to a local runtime\" and put the link of notebook environment (from the console)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPdLm5R70BBM"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    debug = False\n",
        "    env = 'Prod' # Dev (User's GoogleDrive), Prod (AzSL GoogleDrive) or Local (local machine)\n",
        "    csv_path = ''\n",
        "    seed = 44\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    # device = 'tpu' # uncomment to switch to TPU usage\n",
        "\n",
        "    # All 3 Video processing APIs implemented. Use the one that corresponds to your code\n",
        "    video_processing_tool = 'TorchVision' # OpenCV, VidGear or TorchVision\n",
        "\n",
        "    # To have a fixed frame size\n",
        "    max_frames = 64\n",
        "    # The maximum size of the output. This and previous parameters are considered\n",
        "    # for training with the RNN and Transformer networks.\n",
        "    max_words_in_sentence = 10\n",
        "\n",
        "    video_size = 600\n",
        "\n",
        "\n",
        "    drive_folder = '/home/....' # path for the local folder\n",
        "    if (env == 'Dev'):\n",
        "      drive_folder = '/content/drive/MyDrive/SLR_test' # path to your GoogleDrive folder\n",
        "    elif (env == 'Prod'):\n",
        "      drive_folder = '/content/drive/MyDrive/SLR/Data/Datasets/AzSLD' # path to the shared GoogleDrive folder\n",
        "\n",
        "    train_csv_path = drive_folder+'/sentences_all.csv' # where the sentences and glosses are kept\n",
        "    camera_source = 'Cam2' # Cam1 - side-top, Cam2 - front\n",
        "    keep_hands_only = True # If true, will keep only frames with hands\n",
        "    feature_type = 'no' # 'no' (frames only), 'i3d' # or 'squeezenet'\n",
        "\n",
        "    BATCH_SIZE = 128 # updated before the training\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed(seed)\n",
        "\n",
        "config = Config()\n",
        "seed_everything(config.seed)\n",
        "\n",
        "print('Running on',config.device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU number:',torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the TPU configuration. Since it requies some preparation, it is separated from the pre-config code."
      ],
      "metadata": {
        "id": "QZV4h8UVRuQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (config.device == 'tpu'):\n",
        "  !pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl\n",
        "\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "\n",
        "\n",
        "  dev = xm.xla_device()\n",
        "  t1 = torch.ones(3, 3, device = dev)\n",
        "  print(t1)\n",
        "  config.device = dev"
      ],
      "metadata": {
        "id": "44jGu0Aboz6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbEkMcdhlL26"
      },
      "outputs": [],
      "source": [
        "if (config.env != 'Local'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PFKq2YWcT4P"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def pip_install(package):\n",
        "  subprocess.check_call([sys.executable, '-m', 'pip', 'install',package])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4439XWPCpzN4"
      },
      "outputs": [],
      "source": [
        "pip_install('mediapipe')\n",
        "\n",
        "# https://github.com/jbohnslav/opencv_transforms\n",
        "pip_install('opencv_transforms')\n",
        "\n",
        "if config.video_processing_tool == 'VidGear':\n",
        "  pip_install('vidgear[core]')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The video data could be provided as fames or features (i3d or pretrained model)\n",
        "# The code is take from here: https://colab.research.google.com/drive/1LKoytZmNxtC-EuCp7pHDM6sFvK1XdwlW\n",
        "if config.feature_type == 'i3d':\n",
        "  # For I3D features\n",
        "  !git clone https://github.com/v-iashin/video_features.git\n",
        "  !pip install omegaconf==2.0.6\n",
        "\n",
        "  %cd video_features\n",
        "\n",
        "  from models.i3d.extract_i3d import ExtractI3D\n",
        "  from models.raft.raft_src.raft import RAFT, InputPadder\n",
        "  from utils.utils import build_cfg_path\n",
        "  from omegaconf import OmegaConf\n",
        "\n",
        "  # Load and patch the config\n",
        "  args = OmegaConf.load(build_cfg_path(config.feature_type))\n",
        "  # args.show_pred = True\n",
        "  # args.stack_size = 24\n",
        "  # args.step_size = 24\n",
        "  # args.extraction_fps = 30\n",
        "  args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
        "  # args.streams = 'flow'\n",
        "\n",
        "  # Load the model\n",
        "  extractor = ExtractI3D(args)\n",
        "elif config.feature_type == 'squeezenet':\n",
        "  from torchvision.models import squeezenet1_1\n",
        "  from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "  model = squeezenet1_1(pretrained=True).to(config.device)\n",
        "  return_nodes = {\n",
        "        'features.12.cat': 'layer12'\n",
        "        }\n",
        "  pretrained_model = create_feature_extractor(model, return_nodes=return_nodes).to(config.device)\n",
        "  pretrained_model.eval()\n",
        "\n",
        "def frame_to_feats(pretrained_model, frames):\n",
        "  features = pretrained_model(frames.squeeze())['layer12'].to(device=config.device)\n",
        "  feat_shape = features.shape\n",
        "  print('Squeezenet shape',feat_shape)\n",
        "  feat_flat =  torch.reshape(features,(feat_shape[0],feat_shape[1]*feat_shape[2]*feat_shape[3])).to(device=config.device)\n",
        "  return feat_flat"
      ],
      "metadata": {
        "id": "01op-S3rqeLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr8VkJpS2Blo"
      },
      "outputs": [],
      "source": [
        "train_set_size = 500\n",
        "\n",
        "# read cvs file\n",
        "sentences = pd.read_csv(config.train_csv_path)\n",
        "sentences = sentences.iloc[:,:train_set_size]\n",
        "\n",
        "# unique words\n",
        "word_set = set(['SOS','EOS'])\n",
        "sentences.iloc[:,2].str.lower().str.split().apply(word_set.update)\n",
        "sorted_word_set = sorted(word_set)\n",
        "print('Unique words',sorted_word_set)\n",
        "\n",
        "# create word encoding\n",
        "encodings = { k:v for v,k in enumerate(sorted_word_set)}\n",
        "word_idx  = { v:k for k,v in encodings.items()}\n",
        "print('Word encodings',encodings)\n",
        "print('Words by index',word_idx)\n",
        "torch.save(encodings,config.drive_folder+'/encodings.dict')\n",
        "torch.save(word_idx,config.drive_folder+'/word_idx.dict')\n",
        "\n",
        "# converts a sentence with zero padded encoding list\n",
        "def get_sentence_encoded(sentence):\n",
        "    encoded = [encodings[key] for key in ('SOS '+sentence+' EOS').split()]\n",
        "    return  encoded + list([0]) * (config.max_words_in_sentence - len(encoded))\n",
        "\n",
        "if config.debug:\n",
        "  print(get_sentence_encoded('mən hansı sənəd vermək'))\n",
        "  print(get_sentence_encoded('mən bakı yaşamaq'))\n",
        "\n",
        "# generate (video file name, encoding list)\n",
        "df = pd.DataFrame(columns=[\"id\", \"video_file\",\"encoding\"])\n",
        "\n",
        "for index, row in sentences.iterrows():\n",
        "    id = int(row[0])\n",
        "    phrase = row[2].lower()\n",
        "    encoded = get_sentence_encoded(phrase)\n",
        "\n",
        "    dir = config.drive_folder + '/'+ str(id) + '/'+ config.camera_source\n",
        "    # iterate over video folders\n",
        "    for filename in os.listdir(dir):\n",
        "        f = os.path.join(dir, filename)\n",
        "        # checking if it is a file\n",
        "        if os.path.isfile(f):\n",
        "            entry = pd.DataFrame.from_dict({\"id\": id, \"video_file\": f, \"encoding\": [encoded]})\n",
        "            df = pd.concat([df, entry], ignore_index = True)\n",
        "\n",
        "if config.debug:\n",
        "    print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a sample code to visualize the frames (might be used for debugging and validation of the inference"
      ],
      "metadata": {
        "id": "bSPCZPbMSUCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WugbJbtaeYJb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "rows = int(math.sqrt(config.max_frames))\n",
        "cols = config.max_frames//rows\n",
        "\n",
        "def visualize_frames(frames):\n",
        "  fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20,20))\n",
        "\n",
        "  idx = 0\n",
        "  for i in range(rows):\n",
        "      for j in range(cols):\n",
        "        axes[i, j].imshow(frames[0,idx,:,:,:].permute(1,2,0).cpu())\n",
        "        idx += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2UWiRi5w5q8"
      },
      "source": [
        "TODO: Implement augmentation to the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2MWxh1UF1qm"
      },
      "outputs": [],
      "source": [
        "pip_install('pytorchvideo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLf1Rri-QsIE"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    Normalize,\n",
        "    RandomShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    Permute,\n",
        ")\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Lambda,\n",
        "    RandomCrop,\n",
        "    CenterCrop,\n",
        "    RandomAdjustSharpness,\n",
        "    Resize,\n",
        "    ColorJitter,\n",
        "    RandomHorizontalFlip\n",
        ")\n",
        "\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an option, it is possible to keep frames with hands. This might be helpful when there are many 'idle' frames in the beginning or end.\n",
        "\n",
        "The following code performs using the mediapipe's hand detection API. It might slow down the fetching process but if this step is necessary, then the data shall be saved first, and then loaded using a regular dataloader."
      ],
      "metadata": {
        "id": "qSMCeVPWSvFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_frames(video_path,video_transform):\n",
        "  video_data = None\n",
        "\n",
        "  if config.video_processing_tool == 'OpenCV':\n",
        "    video_data = cv2.VideoCapture(video_path)\n",
        "  elif config.video_processing_tool == 'VidGear':\n",
        "    video_data = CamGear(video_path).start()\n",
        "  elif config.video_processing_tool == 'TorchVision':\n",
        "    video_data, _, _ = torchvision.io.read_video(video_path, output_format=\"THWC\")\n",
        "\n",
        "  final_frames = video_data\n",
        "  if config.keep_hands_only:\n",
        "    final_frames = keep_frames_with_hands(video_data, crop_size=config.video_size).to(config.device)\n",
        "\n",
        "  if video_transform:\n",
        "    apply_trans = apply_video_transforms()\n",
        "    hands_only = apply_trans(final_frames)\n",
        "\n",
        "  if config.video_processing_tool == 'OpenCV':\n",
        "    video_data.release()\n",
        "  elif config.video_processing_tool == 'VidGear':\n",
        "    video_data.stop()\n",
        "  elif config.video_processing_tool == 'TorchVision':\n",
        "    pass\n",
        "\n",
        "\n",
        "  n,l,w,h = final_frames.shape\n",
        "\n",
        "  # When frames are more than we need but not that much (just trim it from the start and end)\n",
        "  if (n > config.max_frames) and (n < 2*config.max_frames):\n",
        "    left = (n-config.max_frames)//2\n",
        "    final_frames_new = final_frames[left:(n-left-1),:,:,:]\n",
        "  # If we have much more frames than we need\n",
        "  elif (n > config.max_frames):\n",
        "    # Cut 5 frames from start/end and then skip every n-th\n",
        "    slice_step = ((n-10)//config.max_frames+1)\n",
        "    final_frames_new = final_frames[5:(n-5):slice_step,:,:,:]\n",
        "  else:\n",
        "    final_frames_new = final_frames\n",
        "\n",
        "  n = final_frames_new.shape[0]\n",
        "\n",
        "  # If we have less frames than we need\n",
        "  if (n < config.max_frames):\n",
        "    # fill with the last frames (usually 1)\n",
        "    compliment_arr = hands_only[-(config.max_frames-n):,:,:,:]\n",
        "    final_frames_new = torch.cat((final_frames_new,compliment_arr),0)\n",
        "\n",
        "  return final_frames_new\n",
        "\n",
        "def tensor2list(mdim_tensor):\n",
        "  tensor_stack = []\n",
        "  for i in range(mdim_tensor.shape[0]):\n",
        "    tensor_stack.append(mdim_tensor[i])\n",
        "\n",
        "  return tensor_stack"
      ],
      "metadata": {
        "id": "ATF1mPUDISnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj7JX4ZRTSf7"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "\n",
        "# keeps only informative frames\n",
        "def keep_frames_with_hands(video_data, crop_size: int = None,\n",
        "                           mp_min_detection_confidence: float = 0.8, mp_min_tracking_confidence: float = 0.9):\n",
        "\n",
        "  mpHands = mp.solutions.hands\n",
        "  hands = mpHands.Hands(static_image_mode=True, max_num_hands=2,\n",
        "                        min_detection_confidence=mp_min_detection_confidence, min_tracking_confidence=mp_min_tracking_confidence)\n",
        "\n",
        "  if crop_size:\n",
        "    video_arr = torch.zeros((0, 3, crop_size, crop_size)).to(config.device)\n",
        "    transform = Compose([\n",
        "      CenterCrop(crop_size),\n",
        "      ])\n",
        "  else:\n",
        "    video_arr = torch.zeros((0, 3, 960, 1280)).to(config.device)\n",
        "\n",
        "  ret = True\n",
        "  frame = None\n",
        "\n",
        "  for frame in video_data:\n",
        "    hand_results = hands.process(frame.numpy())\n",
        "\n",
        "    if hand_results.multi_hand_landmarks != None:\n",
        "      if crop_size:\n",
        "        frame_ext = torch.unsqueeze(transform(frame.permute(2, 0, 1)), dim=0).to(config.device)\n",
        "      else:\n",
        "        frame_ext = torch.unsqueeze(frame.permute(2, 0, 1), dim=0).to(config.device)\n",
        "\n",
        "      video_arr = torch.cat((video_arr, frame_ext/255.0),0)\n",
        "\n",
        "\n",
        "  return video_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resize of the video frames done through the following transformations:"
      ],
      "metadata": {
        "id": "BylJ6ynbTbBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMuLQaqQQxbv"
      },
      "outputs": [],
      "source": [
        "def apply_video_transforms(resize_size: int = config.video_size):\n",
        "    video_transform=Compose([\n",
        "        Resize(size=(resize_size, resize_size))\n",
        "    ])\n",
        "\n",
        "    return video_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCKhH_5Zzt3D"
      },
      "outputs": [],
      "source": [
        "if config.video_processing_tool == 'VidGear':\n",
        "  from vidgear.gears import CamGear\n",
        "import torchvision\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "class SLDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, video_transform: bool = True):\n",
        "        # shuffle and save\n",
        "        self.df = sklearn.utils.shuffle(df)\n",
        "        self.video_transform = video_transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if config.debug:\n",
        "          print(f\"Got item at index: {idx}\")\n",
        "        video_path = df.iloc[idx,1]\n",
        "\n",
        "        encoding = torch.tensor(df.iloc[idx,2]).to(config.device)\n",
        "        enc_shape = encoding.shape[0]\n",
        "\n",
        "        if config.feature_type == 'i3d':\n",
        "          feature_dict = extractor.extract(f)\n",
        "\n",
        "          f_num, f_size = feature_dict['rgb'].shape\n",
        "          REQ_FEATS = 5 # required number of features\n",
        "\n",
        "          # Keep only REQ_FEATS features from each and apply zero padding if there are less than REQ_FEATS features\n",
        "          feats_rgb = torch.from_numpy(feature_dict['rgb'])\n",
        "          feats_flow = torch.from_numpy(feature_dict['flow'])\n",
        "\n",
        "          # Trim extra features.\n",
        "          # Trim shall be applied on each, since we need to have equal number of RGB and FLOW features.\n",
        "          # Like for RGB and FLOW, 8 features each will make 16 features if we apply catenation first.\n",
        "          # If we trimming after that to keep 10 features, eight of them will be about RGB, two - FLOW.\n",
        "          if f_num > REQ_FEATS:\n",
        "            feats_rgb  = feats_rgb[-(REQ_FEATS-f_num):,:]\n",
        "            feats_flow = feats_flow[-(REQ_FEATS-f_num):,:]\n",
        "\n",
        "          # Concatenate the features\n",
        "          feats = torch.cat((feats_rgb,feats_flow),1)\n",
        "\n",
        "          # Apply zero padding if needed.\n",
        "          # Zero padding needs to be done after the catenation - zero features shall come at the end, not after each type (RGB and FLOW)\n",
        "          if f_num < REQ_FEATS:\n",
        "            padarr = torch.zeros((REQ_FEATS-f_num,f_size*2))\n",
        "            feats = torch.cat((feats,padarr),0)\n",
        "        else:\n",
        "          frames = get_video_frames(f,self.video_transform)\n",
        "          if config.feature_type == 'no':\n",
        "            feats = frames\n",
        "          else:\n",
        "            feats = frame_to_feats(pretrained_model,frames)\n",
        "          print('feat shape:',feats.shape)\n",
        "\n",
        "        return feats, torch.reshape(encoding,(enc_shape,1)),video_path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "def get_dataloader(df, phase: str, batch_size: int = 96) -> DataLoader:\n",
        "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=config.seed, stratify=df['id'])\n",
        "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
        "    df = train_df if phase == 'train' else val_df\n",
        "    dataset = SLDataset(df, video_transform=True)\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "dl = get_dataloader(df,'train',1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgOLGv7gYYi5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "measure = time.time()\n",
        "dl_next = next(iter(dl))\n",
        "print('Data fetching:',time.time() - measure,'sec')\n",
        "\n",
        "a,b,fname = dl_next\n",
        "\n",
        "if config.debug:\n",
        "  print(a.shape,b.shape,fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch one and visualize"
      ],
      "metadata": {
        "id": "dnx546lvT1z3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG3cfSx_U3Bd"
      },
      "outputs": [],
      "source": [
        "print(a.shape,b.shape,fname)\n",
        "if config.feature_type == 'no':\n",
        "  visualize_frames(a)\n",
        "\n",
        "# No: torch.Size([1, 64, 3, 600, 600]) torch.Size([1, 10, 1]) ('drive/MyDrive/SLR/Data/Video/Cam2/362/2022-12-22 16-32-52.mp4',)\n",
        "# i3d: torch.Size([1, 5, 2048]) torch.Size([1, 10, 1]) ('/content/drive/MyDrive/SLR/Data/Video/Cam2/315/2022-11-04 14-23-26.mp4',)\n",
        "# squeezenet: torch.Size([1, 64, 700928]) torch.Size([1, 10, 1]) ('/content/drive/MyDrive/SLR/Data/Video/Cam2/342/2022-12-23 17-17-50.mp4',)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}